## Workshop 4: **Accelerate BERT Inference with Knowledge Distillation & AWS Inferentia**

In the workshop, you will learn how to apply knowledge distillation to compress a large model to a small model, and then from the small model to an optimized neuron model with AWS Inferentia. By the end of this process, our model will go from 100ms+ to 5ms+ latency - a 20x improvement! 🤯 🏎


- Apply knowledge-distillation with BERT-large as teacher and MiniLM as student
- Compile a Hugging Face Transformer model with AWS Neuron for AWS Inferentia
- Deploy the distilled & optimized model to Amazon SageMaker for production-grade fast inference


---

🧑🏻‍💻 Code Assets: [workshop_4_distillation_and_acceleration](https://github.com/philschmid/huggingface-sagemaker-workshop-series/tree/main/workshop_4_distillation_and_acceleration)

📺 Youtube: _Coming Soon_