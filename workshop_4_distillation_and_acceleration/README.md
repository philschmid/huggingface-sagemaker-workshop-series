## Workshop 4: **Accelerate BERT Inference with Knowledge Distillation & AWS Inferentia**

In the workshop, you will learn how to apply knowledge distillation to compress a large model to a small model, and then from the small model to an optimized neuron model with AWS Inferentia. By the end of this process, our model will go from 100ms+ to 5ms+ latency - a 20x improvement! ğŸ¤¯ ğŸ


- Apply knowledge-distillation with BERT-large as teacher and MiniLM as student
- Compile a Hugging Face Transformer model with AWS Neuron for AWS Inferentia
- Deploy the distilled & optimized model to Amazon SageMaker for production-grade fast inference


---

ğŸ§‘ğŸ»â€ğŸ’» Code Assets: [workshop_4_distillation_and_acceleration](https://github.com/philschmid/huggingface-sagemaker-workshop-series/tree/main/workshop_4_distillation_and_acceleration)

ğŸ“ºÂ Youtube: https://www.youtube.com/watch?v=3fulTyMXhWQ&ab_channel=HuggingFace
