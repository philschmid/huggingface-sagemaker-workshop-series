{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Models Smaller via Knowledge Distillation\n",
    "\n",
    "### A text classification example using Hugging Face Transformers and Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to our end-to-end example of _knowledge distillation_ using Hugging Face Transformers & Amazon SageMaker! This example is adapted from Chapter 8 of the O'Reilly book [_Natural Language Processing with Transformers_](https://transformersbook.com/).\n",
    "\n",
    "Knowledge distillation is a general-purpose method for training a smaller student model to mimic the behavior of a slower, larger, but better-performing teacher. Originally introduced in 2006 in the context of ensemble models ([link](https://dl.acm.org/doi/10.1145/1150402.1150464)), it was later popularized in a famous [2015 paper](https://arxiv.org/abs/1503.02531) that generalized the method to deep neural networks and applied it to image classification and automatic speech recognition.\n",
    "\n",
    "Given the trend toward pretraining language models with ever-increasing parameter counts (the largest at the time of writing having over one trillion parameters), knowledge distillation has also become a popular strategy to compress these huge models and make them more suitable for building practical applications.\n",
    "\n",
    "In this notebook, we will use _intent detection_ as a case study (a form of text classification). This is an important component of text-based assistants, where low latencies are critical for maintaining a conversation in real time. We'll use the [CLINC150](https://huggingface.co/datasets/clinc_oos) dataset for training, and we've already fine-tuned a [RoBERTa-large model]((https://huggingface.co/optimum/roberta-large-finetuned-clinc) that will act as the teacher. Our goal will be to compress the knowledge of this teacher into a much smaller student called [MiniLM](https://huggingface.co/nreimers/MiniLMv2-L12-H384-distilled-from-RoBERTa-Large)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge distillation for fine-tuning\n",
    "\n",
    "So how is knowledge actually \"distilled\" or transferred from the teacher to the student during training? For supervised tasks like fine-tuning, the main idea is to augment the ground truth labels with a distribution of \"soft probabilities\" from the teacher which provide complementary information for the student to learn from. For example, if our teacher assigns high probabilities to multiple intents, then this could be a sign that these intents lie close to each other in the feature space. By training the student to mimic these probabilities, the goal is to distill some of this \"dark knowledge\" that the teacher has learned—that is, knowledge that is not available from the labels alone. \n",
    "\n",
    "Mathematically, the way this works is as follows. Suppose we feed an input sequence $x$ to the teacher to generate a vector of logits ${\\bf z}(x)$ = $[z_1(x), \\ldots , z_N(x)]$. We can convert these logits into probabilities by applying a softmax function:\n",
    "\n",
    "$$\\frac{\\exp(z_i(x))}{\\sum_j \\exp(z_i(x))} \\,$$\n",
    "\n",
    "This isn't quite what we want, though, because in many cases the teacher will assign a high probability to one class, with all other class probabilities close to zero. When that happens, the teacher doesn't provide much additional information beyond the ground truth labels, so instead we \"soften\" the probabilities by scaling the logits with a temperature hyperparameter $T$ before applying the softmax:\n",
    "\n",
    "$$ p_i(x) = \\frac{\\exp(z_i(x) / T)}{\\sum_j \\exp(z_i(x) / T)} \\,$$\n",
    "\n",
    "As shown in the figure below, higher values of $T$ produce a softer probability distribution over the classes and reveal much more information about the decision boundary that the teacher has learned for each training example. When $T=1$ we recover the original softmax distribution.\n",
    "\n",
    "![kkd](./assets/soft-probs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the student also produces softened probabilities $q_i(x)$ of its own, we can use the [Kullback–Leibler (KL)](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) divergence to measure the difference between the two probability distributions:\n",
    "\n",
    "$$ D_{KL}(p, q) =  \\sum_i p_i(x) \\log \\frac{p_i(x)}{q_i(x)} \\,$$\n",
    "\n",
    "With the KL divergence we can calculate how much is lost when we approximate the probability distribution of the teacher with the student. This allows us to define a knowledge distillation loss:\n",
    "\n",
    "$$ L_{KD} = T^2 D_{KL} \\,$$\n",
    "\n",
    "where $T^2$ is a normalization factor to account for the fact that the magnitude of the gradients produced by soft labels scales as $1/T^2$. For classification tasks, the student loss is then a weighted average of the distillation loss with the usual cross-entropy loss $L_{CE}$ of the ground truth labels:\n",
    "\n",
    "$$ L_\\mathrm{student} = \\alpha L_{CE} + (1-\\alpha)  L_{KD} \\,$$\n",
    "\n",
    "where $\\alpha$ is a hyperparameter that controls the relative strength of each loss. A diagram of the whole process is shown below; the temperature is set to 1 at inference time to recover the standard softmax probabilities.\n",
    "\n",
    "![kkd](./assets/task-specific-kd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge distillation for pretraining\n",
    "\n",
    "Knowledge distillation can also be used during pretraining to create a general-purpose student that can be subsequently fine-tuned on downstream tasks. In this case, the teacher is a pretrained language model like BERT, which transfers its knowledge about masked language modeling to the student. For example, in the [DistilBERT paper](https://arxiv.org/abs/1910.01108), the masked language modeling loss $L_{mlm}$ is augmented with a term from knowledge distillation and a cosine embedding loss $L_{cos} = 1 - \\cos(h_s,h_t)$ to align the directions of the hidden state vectors between the teacher and student:\n",
    "\n",
    "$$ L_\\mathrm{DistilBERT}  = \\alpha L_{mlm} + \\beta L_{KD} + \\gamma L_{cos}\\,$$\n",
    "\n",
    "In the literature, these two types of knowledge distillation are often called _task-specific_ (finetuning) and _task-agnostic_ (pretraining) knowledge distillation.\n",
    "\n",
    "![knowledge-distillation](./assets/knowledge-distillation.png)\n",
    "_Task-specific distillation (left) versus task-agnostic distillation (right). Figure from FastFormers by Y. Kim and H. Awadalla [arXiv:2010.13382]._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are more interested in those topics you should defintely read: \n",
    "* [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)\n",
    "* [FastFormers: Highly Efficient Transformer Models for Natural Language Understanding](https://arxiv.org/abs/2010.13382)\n",
    "\n",
    "Especially the [FastFormers paper](https://arxiv.org/abs/2010.13382) contains great research on what works and doesn't work when using knowledge distillation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install sagemaker huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will use the [Hugging Face Hub](https://huggingface.co/models) as remote model versioning service. To be able to push our model to the Hub, you need to register on the [Hugging Face](https://huggingface.co/join). \n",
    "If you already have an account you can skip this step. \n",
    "After you have an account, we will use the `notebook_login` util from the `huggingface_hub` package to log into our account and store our token (access key) on the disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /home/ec2-user/.huggingface/token\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration\n",
    "\n",
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker bucket: sagemaker-us-east-1-558105141721\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DistillationTrainer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Normally, when fine-tuning a transformer model using PyTorch you should go with the `Trainer-API`. The [Trainer](https://huggingface.co/docs/transformers/v4.16.1/en/main_classes/trainer#transformers.Trainer) class provides an API for feature-complete training in PyTorch for most standard use cases. \n",
    "\n",
    "In our example we cannot use the `Trainer` out-of-the-box, since we need to pass in two models, the `Teacher` and the `Student` and compute the loss for both. But we can subclass the `Trainer` to create a `DistillationTrainer` which will take care of it and only overwrite the [compute_loss](https://github.com/huggingface/transformers/blob/c4ad38e5ac69e6d96116f39df789a2369dd33c21/src/transformers/trainer.py#L1962) method as well as the `init` method. In addition to this we also need to subclass the `TrainingArguments` to include the our distillation hyperparameters. \n",
    "\n",
    "The [DistillationTrainer](https://github.com/philschmid/knowledge-distillation-transformers-pytorch-sagemaker/blob/e8d04240d0ebbd7bd0741d196e8902a69a34b414/scripts/train.py#L28) and [DistillationTrainingArguments](https://github.com/philschmid/knowledge-distillation-transformers-pytorch-sagemaker/blob/e8d04240d0ebbd7bd0741d196e8902a69a34b414/scripts/train.py#L21) are directly integrated into [training script](./scripts/train.py)\n",
    "\n",
    "```python\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        # place teacher on same device as student\n",
    "        self._move_model_to_device(self.teacher, self.model.device)\n",
    "        self.teacher.eval()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "        # compute student output\n",
    "        outputs_student = model(**inputs)\n",
    "        student_loss = outputs_student.loss\n",
    "        # compute teacher output\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher(**inputs)\n",
    "\n",
    "        # assert size\n",
    "        assert outputs_student.logits.size() == outputs_teacher.logits.size()\n",
    "\n",
    "        # Soften probabilities and compute distillation loss\n",
    "        loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        loss_logits = (\n",
    "            loss_function(\n",
    "                F.log_softmax(outputs_student.logits / self.args.temperature, dim=-1),\n",
    "                F.softmax(outputs_teacher.logits / self.args.temperature, dim=-1),\n",
    "            )\n",
    "            * (self.args.temperature ** 2)\n",
    "        )\n",
    "        # Return weighted student loss\n",
    "        loss = self.args.alpha * student_loss + (1.0 - self.args.alpha) * loss_logits\n",
    "        return (loss, outputs_student) if return_outputs else loss\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Creating an Estimator with our Teacher & Student Model\n",
    "\n",
    "In this example, we will use [RoBERTa-Large](https://huggingface.co/roberta-large) as Teacher and [MiniLM](https://huggingface.co/nreimers/MiniLMv2-L12-H384-distilled-from-RoBERTa-Large) as Student. Our Teacher is already fine-tuned on our dataset, which makes it easy for us to directly start the distillation training job rather than fine-tuning the teacher first to then distill it afterwards.\n",
    "_**IMPORTANT**: This example will only work with a `Teacher` & `Student` combination where the Tokenizer is creating the same output._\n",
    "\n",
    "Additionally, describes the [FastFormers: Highly Efficient Transformer Models for Natural Language Understanding](https://arxiv.org/abs/2010.13382) paper an additional phenomenon. \n",
    "> In our experiments, we have observed that dis-\n",
    "tilled models do not work well when distilled to a\n",
    "different model type. Therefore, we restricted our\n",
    "setup to avoid distilling RoBERTa model to BERT\n",
    "or vice versa. The major difference between the\n",
    "two model groups is the input token (sub-word) em-\n",
    "bedding. We think that different input embedding\n",
    "spaces result in different output embedding spaces,\n",
    "and knowledge transfer with different spaces does\n",
    "not work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'teacher_id':'optimum/roberta-large-finetuned-clinc',           \n",
    "    'student_id':'nreimers/MiniLMv2-L12-H384-distilled-from-RoBERTa-Large',           \n",
    "    'dataset_id':'clinc_oos',           \n",
    "    'dataset_config':'plus',             \n",
    "    'epochs': 10,             \n",
    "    # distillation parameter\n",
    "    'alpha': 0.055199695773231194, # 0.5,\n",
    "    'temparature': 19, # 4 \n",
    "    'learning_rate': 1e-4, # 3e-5\n",
    "    # hpo parameter\n",
    "    \"run_hpo\": False,\n",
    "    \"n_trials\": 100,\n",
    "    # push to hub config\n",
    "    'push_to_hub': True,                            \n",
    "    'hub_model_id': 'MiniLMv2-L12-H384-distilled-finetuned-clinc', \n",
    "    'hub_token': HfFolder.get_token()               \n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'knowledge_distillation.py',        \n",
    "    source_dir           = './scripts',       \n",
    "    instance_type        = 'ml.p3.2xlarge',   \n",
    "    instance_count       = 1,                 \n",
    "    role                 = role,              \n",
    "    transformers_version = '4.17',            \n",
    "    pytorch_version      = '1.10',             \n",
    "    py_version           = 'py38',            \n",
    "    hyperparameters      = hyperparameters,   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start our Training with Knowledge-Distillation and Hyperparamter optimization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "# setting wait to False to not expose the HF Token\n",
    "huggingface_estimator.fit(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using the Hugging Face Hub intergration with Tensorboard we can inspect our progress directly on the hub, as well as testing checkpoints during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/philschmid/minilm-l12-h384-sst2-distilled\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "whoami = HfApi().whoami()\n",
    "username = whoami['name']\n",
    "\n",
    "print(f\"https://huggingface.co/{username}/{hyperparameters['hub_model_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to achieve a `accuracy` of 0.8337, which is a very good result for our model. Our distilled `Tiny-Bert` has 96% less parameters than the teacher `bert-base` and runs ~46.5x faster while preserving over 90% of BERT’s performances as measured on the SST2 dataset.\n",
    "\n",
    "| model | Parameter | Speed-up | Accuracy |\n",
    "|------------|-----------|----------|----------|\n",
    "| BERT-base  | 109M      | 1x       | 93%      |\n",
    "| tiny-BERT  | 4M        | 46.5x    | 83%      |\n",
    "\n",
    "_Note: The [FastFormers paper](https://arxiv.org/abs/2010.13382) uncovered that the biggest boost in performance is observerd when having 6 or more layers in the student. The [google/bert_uncased_L-2_H-128_A-2](https://huggingface.co/google/bert_uncased_L-2_H-128_A-2) we used only had 2, which means when changing our student to, e.g. `distilbert-base-uncased` we should better performance in terms of accuracy._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.c5.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint. We will send 1000 sounds request with a sequence length of 128 to get a estimation of the latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_input= {\"inputs\": \"Harry believes it, although no one else believes that Sally is innocent.\" * 9} # generates 128 seq length input\n",
    "\n",
    "for i in range(1000):\n",
    "  predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a look at cloudwatch to get our monitoring metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'ModelLatency~'EndpointName~'huggingface-pytorch-inference-2022-04-11-08-59-13-689~'VariantName~'AllTraffic))~view~'timeSeries~stacked~false~region~'us-east-1~start~'-PT10M~end~'P0D~stat~'p99~period~300);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20huggingface-pytorch-inference-2022-04-11-08-59-13-689\n"
     ]
    }
   ],
   "source": [
    "print(f\"https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'ModelLatency~'EndpointName~'{predictor.endpoint_name}~'VariantName~'AllTraffic))~view~'timeSeries~stacked~false~region~'us-east-1~start~'-PT10M~end~'P0D~stat~'p99~period~300);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20{predictor.endpoint_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Performance chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from visualize import plot_metrics\n",
    "%matplotlib inline\n",
    "\n",
    "metrics = {\n",
    "  \"roberta-large\": {\"time_p99_ms\": 322, \"accuracy\": 0.9644,\"size_mb\":1322},\n",
    "  \"minilm-12-h384\": {\"time_p99_ms\": 79, \"accuracy\": 0.9220,\"size_mb\":156},\n",
    "  }\n",
    "\n",
    "plot_metrics(metrics, \"minilm-12-h384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "ec1370a512a4612a2908be3c3c8b0de1730d00dc30104daff827065aeaf438b7"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
